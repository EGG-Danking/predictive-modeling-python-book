# -*- coding: utf-8 -*-
"""Spam Detection (Binary Classification) - SOLUTIONS

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kzyfv_4qOug8e37uCTdvqA_HpYvuUyev
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install ucimlrepo

"""## Data Loading

http://archive.ics.uci.edu/dataset/94/spambase
"""

from ucimlrepo import fetch_ucirepo

# http://archive.ics.uci.edu/dataset/94/spambase
ds = fetch_ucirepo(id=94)

#ds.metadata

ds.variables

df = ds["data"]["original"]
df.rename(columns={'Class':'is_spam'}, inplace=True) # spam (1) or not (0)
#df["is_spam"] = df["is_spam"].map({0: "Not", 1: "Spam"})
print(df.shape)
df.head()

"""Target:

  + `is_spam`: denotes whether the e-mail was considered spam (i.e. unsolicited commercial e-mail) (1) or not (0).  

Features:

Most of the attributes indicate whether a particular word or character was frequently occuring in the e-mail.  


  + 48 x `word_freq_WORD` : percentage of words in the e-mail that match WORD, i.e. 100 * (number of times the WORD appears in the e-mail) / total number of words in e-mail.  A "word" in this case is any string of alphanumeric characters bounded by non-alphanumeric characters or end-of-string.

  + 6 x `char_freq_CHAR` : percentage of characters in the e-mail that match CHAR, i.e. 100 * (number of CHAR occurences) / total characters in e-mail

The run-length attributes measure the length of sequences of consecutive capital letters.  

  + `capital_run_length_average` : average length of uninterrupted sequences of capital letters

  + `capital_run_length_longest` : length of longest uninterrupted sequence of capital letters

  + `capital_run_length_total` : sum of length of uninterrupted sequences of capital letters. total number of capital letters in the e-mail

"""

#df.isna().sum()

print(len(df.columns))
#df.columns.tolist()

#df.isna().sum()

"""## Data Exploration"""

df["is_spam"].value_counts() # imbalanced. we might need to choose a sampling strategy

df["is_spam"].isna().sum()

"""### Correlation"""

#import plotly.express as px
#
#def plot_correlation_matrix(df, method="pearson", height=950):
#    """Params: method (str): "spearman" or "pearson". """
#
#    cor_mat = df.corr(method=method, numeric_only=True)
#
#    title= f"{method.title()} Correlation between Economic Indicators"
#
#    fig = px.imshow(cor_mat,
#                    height=height, # title=title,
#                    text_auto= ".2f", # round to two decimal places
#                    color_continuous_scale="Blues",
#                    color_continuous_midpoint=0,
#                    labels={"x": "Indicator", "y": "Indicator"},
#    )
#    # center title (h/t: https://stackoverflow.com/questions/64571789/)
#    fig.update_layout(title={'text': title, 'x':0.485, 'xanchor': 'center'})
#    fig.show()
#
#
#plot_correlation_matrix(df, method="spearman", height=1500)

corr_target = df.corr()["is_spam"].sort_values(ascending=False)
corr_target.head()

corr_target.tail()

"""## X/Y Split"""

target = "is_spam"
x = df.drop(columns=target)
y = df[target]

print("X:", x.shape)
print("Y:", y.shape)

#x

"""## Feature Scaling"""

# x.describe().T[["min", "max"]]

#x.max().sort_values(ascending=False)

x_scaled = (x - x.mean(axis=0)) / x.std(axis=0)
x_scaled.describe().T[["mean", "std"]]

"""## Train Test Split"""

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x_scaled, y,
                                                    test_size=0.2,
                                                    random_state=99)
print("TRAIN:", x_train.shape, y_train.shape)
print("TEST:", x_test.shape, y_test.shape)

"""## Model Training"""

from sklearn.linear_model import LogisticRegression

model = LogisticRegression(random_state=99)
model.fit(x_train, y_train)

"""Examining coeficients:"""

model.coef_.shape

from pandas import Series

coef = Series(model.coef_[0], index=x_train.columns)
coef.sort_values(ascending=False)

"""## Model Eval"""

y_pred = model.predict(x_test)

from sklearn.metrics import classification_report

print(classification_report(y_test, y_pred))

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

accy = accuracy_score(y_test, y_pred)
print("ACCY:", round(accy,3))

prec = precision_score(y_test, y_pred)
print("PRECISION:", round(prec,3))

rec = recall_score(y_test, y_pred)
print("RECALL:", round(rec,3))

f1 = f1_score(y_test, y_pred)
print("F1:", round(f1,3))

cls_rpt = classification_report(y_test, y_pred, output_dict=True)
cls_rpt

accy = cls_rpt["accuracy"]

print("ACCY:", round(accy,3))

print("---------")
score_type = "weighted avg"
print(score_type.upper(), "...")

precision = cls_rpt[score_type]["precision"]
print("PRECISION:", round(precision, 3))

recall = cls_rpt[score_type]["recall"]
print("RECALL:", round(recall,3))

f1 = cls_rpt[score_type]["f1-score"]
print("F1:", round(f1,3))



print("---------")
score_type = "macro avg"
print(score_type.upper(), "...")

precision = cls_rpt[score_type]["precision"]
print("PRECISION:", round(precision, 3))

recall = cls_rpt[score_type]["recall"]
print("RECALL:", round(recall,3))

f1 = cls_rpt[score_type]["f1-score"]
print("F1:", round(f1,3))

"""### Confusion Matrix"""

from sklearn.metrics import confusion_matrix
# Confusion matrix whose i-th row and j-th column
# ... indicates the number of samples with
# ... true label being i-th class (ROW)
# ... and predicted label being j-th class (COL)

confusion_matrix(y_test, y_pred)

class_names = sorted(y_test.unique().tolist())
print(class_names)
confusion_matrix(y_test, y_pred, labels=class_names)

class_names = sorted(y_test.unique().tolist(), reverse=True)
print(class_names)
confusion_matrix(y_test, y_pred, labels=class_names)

from sklearn.metrics import confusion_matrix
import plotly.express as px

def plot_confusion_matrix(y_true, y_pred, height=450, showscale=False, title=None, subtitle=None):
    # https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html
    # Confusion matrix whose i-th row and j-th column
    # ... indicates the number of samples with
    # ... true label being i-th class
    # ... and predicted label being j-th class
    cm = confusion_matrix(y_true, y_pred)

    class_names = sorted(y_test.unique().tolist())

    cm = confusion_matrix(y_test, y_pred, labels=class_names)

    title = title or "Confusion Matrix"
    if subtitle:
        title += f"<br><sup>{subtitle}</sup>"

    fig = px.imshow(cm, x=class_names, y=class_names, height=height,
                    labels={"x": "Predicted", "y": "Actual"},
                    color_continuous_scale="Blues", text_auto=True,
    )
    fig.update_layout(title={'text': title, 'x':0.485, 'xanchor': 'center'})
    fig.update_coloraxes(showscale=showscale)

    fig.show()


subtitle = f"Spam Classification Model: {model.__class__.__name__}"
plot_confusion_matrix(y_test, y_pred, subtitle=subtitle)

"""### ROC-AUC"""

y_pred_proba = model.predict_proba(x_test)
print(y_pred_proba.shape)

from sklearn.metrics import roc_auc_score

def compute_roc_auc_score(y_test, y_pred_proba, is_multiclass=False):
    """NOTE: roc_auc_score uses average='macro' by default"""

    if is_multiclass:
        return roc_auc_score(y_true=y_test, y_score=y_pred_proba, multi_class="ovr")
    else:
        y_pred_proba_pos = y_pred_proba[:,1] # positive class (for binary classification)
        return roc_auc_score(y_true=y_test, y_score=y_pred_proba_pos)



compute_roc_auc_score(y_test, y_pred_proba)